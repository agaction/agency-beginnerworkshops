{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 2 Answer Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_data(path):\n",
    "    f = open(path, 'r')\n",
    "    \n",
    "    lines = f.readlines()\n",
    "    \n",
    "    training_images = np.zeros((len(lines), 784))\n",
    "    training_labels = np.zeros((len(lines), 10))\n",
    "    index = 0\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        label = int(line[0])\n",
    "        training_images[index, :] = np.fromstring(line[2:], dtype=int, sep=',')\n",
    "        training_labels[index, label - 1] = 1.0\n",
    "        index += 1\n",
    "        \n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    return training_images / 255, training_labels\n",
    "\n",
    "training_images, training_labels = get_data(\"mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \"\"\"\n",
    "    A Fully Connected Neural Network. There are 784 input layer nodes, 12 hidden layer nodes, and 10 output layer\n",
    "    nodes.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        # We need to create arrays to store all of the parts of our network:\n",
    "        \n",
    "        # Arrays to hold node values\n",
    "        self.N = np.zeros((784, ))\n",
    "        self.H = np.zeros((12, ))\n",
    "        self.Z = np.zeros((10, ))\n",
    "        \n",
    "        # Arrays to hold weight values (randomly initialized between -1 and 1)\n",
    "        self.W = 2 * np.random.rand(784, 12) - 1\n",
    "        self.V = 2 * np.random.rand(12, 10) - 1\n",
    "        \n",
    "        # Arrays to hold biases for hidden and output nodes (randomly initialized between -1 and 1)\n",
    "        self.B = 2 * np.random.rand(12) - 1\n",
    "        self.C = 2 * np.random.rand(10) - 1\n",
    "        \n",
    "\n",
    "    def fill_input_nodes(self, x):\n",
    "        \"\"\"\n",
    "        Given an image vector, fill self.N, the input node array. Remember, we just put values from the image\n",
    "        into the input nodes, so this should be very easy.\n",
    "        \n",
    "        Parameters:\n",
    "        x: input vector representing image data, one-dimensional vector\n",
    "        \"\"\"\n",
    "        \n",
    "        self.N = x\n",
    "        \n",
    "    \n",
    "    def calculate_hi(self, i): \n",
    "        \"\"\"\n",
    "        Assuming the input nodes array is full, use these values to calculate hi, the ith hidden layer node.\n",
    "        Once the value is calculated, fill the corresponding entry in self.H.\n",
    "        You will need to access weight array W and bias array B.\n",
    "        \n",
    "        Parameters:\n",
    "        i: the index telling which hidden layer node to calculate\n",
    "        \"\"\"\n",
    "        \n",
    "        h = np.dot(self.N, self.W[:, i]) + self.B[i]\n",
    "        h = np.tanh(h) # apply the tanh activation function\n",
    "        self.H[i] = h\n",
    "        \n",
    "        # Some of you used a for loop to multiply values from the input layer by the appropriate weights. \n",
    "        # While that is an acceptable solution, it is faster to use a numpy dot product (numpy operations\n",
    "        # are faster than python loops).\n",
    "\n",
    "    \n",
    "    def fill_hidden_nodes(self):\n",
    "        \"\"\"\n",
    "        Use the calculate_hi method to iteratively fill every hidden layer node. This should be easy if your\n",
    "        calculate_hi method works.\n",
    "        \n",
    "        For thought:\n",
    "        Finding each hi value individually is a perfectly acceptable way of calculating node values,\n",
    "        but is it the most efficient? Is there a way to calculate all hi values at once?\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(12):\n",
    "            self.calculate_hi(i)\n",
    "        \n",
    "        \n",
    "    def calculate_zi(self, i):\n",
    "        \"\"\"\n",
    "        Assuming the hidden nodes array is full, use these values to calculate zi, the ith output layer node.\n",
    "        Once the value is calculated, fill the corresponding entry in self.Z.\n",
    "        You will need to access weight array V and bias array C.\n",
    "        \n",
    "        Parameters:\n",
    "        i: the index telling which output layer node to calculate\n",
    "        \"\"\"\n",
    "        \n",
    "        z = np.dot(self.H, self.V[:, i]) + self.C[i]\n",
    "        z = 1 / (1 + np.exp(-1 * z))  # applying the sigmoid activation function\n",
    "        self.Z[i] = z\n",
    "\n",
    "        \n",
    "    def fill_output_nodes(self):\n",
    "        \"\"\"\n",
    "        Use the calculate_zi method to iteratively fill every output layer node. This should be easy if your\n",
    "        calculate_zi method works.\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(10):\n",
    "            self.calculate_zi(i)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Given an image vector, fill every node in the network. You have already written the necesary methods to\n",
    "        complete this task, you just need to call them in the right order.\n",
    "        \n",
    "        Parameters:\n",
    "        x: input vector representing image data, one-dimensional vector\n",
    "        \"\"\"\n",
    "        \n",
    "        self.fill_input_nodes(x)\n",
    "        self.fill_hidden_nodes()\n",
    "        self.fill_output_nodes()\n",
    "        \n",
    "        \n",
    "\n",
    "    ### Challenge 1\n",
    "    # You've managed to complete the forward pass. However, the way we implemented the forward pass just now was\n",
    "    # inefficient. There is actually a way to calculate values layer-by-layer instead of node-by-node. Can you\n",
    "    # think of a formula that can calculate all hidden nodes at once, instead of one-at-a-time? \n",
    "    #\n",
    "    # Hint: you will need to do matrix operations between the entire weight array, input array, and bias array \n",
    "    #       instead of just a column or element\n",
    "    \n",
    "    def fill_hidden_nodes_fast(self):\n",
    "        \"\"\"\n",
    "        Assume the input nodes array is filled. Now, use these values to fill all hidden nodes at once.\n",
    "        You will need to use self.N, self.W, and self.B.\n",
    "        This should only take up a few lines\n",
    "        \"\"\"\n",
    "\n",
    "        H = np.dot(self.N, self.W) + self.B\n",
    "        self.H = np.tanh(H)\n",
    "\n",
    "        \n",
    "    def fill_output_nodes_fast(self):\n",
    "        \"\"\"\n",
    "        Assume the hidden nodes array is filled. Now, use these values to fill all output nodes at once.\n",
    "        You will need to use self.H, self.V, and self.C.\n",
    "        This should only take up a few lines.\n",
    "        \"\"\"\n",
    "\n",
    "        Z = np.dot(self.H, self.V) + self.C\n",
    "        self.Z = 1 / (1 + np.exp(-1 * Z))\n",
    "\n",
    "    \n",
    "    def forward_fast(self, x):\n",
    "        \"\"\"\n",
    "        Given an image vector, fill every node in the network using the more efficient methods you just wrote.\n",
    "        \n",
    "        Parameters:\n",
    "        x: input vector representing image data, one-dimensional vector\n",
    "        \"\"\"\n",
    "        \n",
    "        self.fill_input_nodes(x)\n",
    "        self.fill_hidden_nodes_fast()\n",
    "        self.fill_output_nodes_fast()\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    ### Challenge 2\n",
    "    # Ok, so you've completed forward pass for real this time. But what do we do with this information?\n",
    "    # If we want our network to learn anything, we'll need to use the outputs from forward pass to generate\n",
    "    # a loss, or error, that measures how far away from the target our network was.\n",
    "    #\n",
    "    # The process is simple: forward pass an image through the network, then read the output.\n",
    "    # Compare this output to the label corresponding to that image. These will both be one-dimensional\n",
    "    # arrays of the same size. The most simple loss function is just calculating the distance between these\n",
    "    # two vectors. \n",
    "    \n",
    "    def calculate_loss(self, x, y):\n",
    "        \"\"\"\n",
    "        Given an image vector and its corresponding label vector, calculate the loss.\n",
    "        \n",
    "        Parameters:\n",
    "        x: input vector representing image data, one-dimensional vector\n",
    "        y: input vector representing label, one-dimensional vector. Has a 1 in the position corresponding to the\n",
    "           correct answer, and 0s everywhere else.\n",
    "        \"\"\"\n",
    "        \n",
    "        out = self.forward_fast(x)\n",
    "        loss = np.sum((self.Z - y) ** 2)\n",
    "        print(loss) # we will just print loss for now. Later we may need to save it for gradient descent.\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code here in the space below. Feel free to make as many additional cells as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample from the dataset to test data\n",
    "sample_image = training_images[0]\n",
    "sample_label = training_labels[0]\n",
    "\n",
    "# create a NeuralNetwork object and test the .forward() method\n",
    "net = NeuralNetwork()\n",
    "net.forward(sample_image)\n",
    "print(net.Z) # this should be a one-dimensional array of length 10. The output will be random and meaningless\n",
    "             # for now because we have not done any backpropagation\n",
    "    \n",
    "print(\"\\n\")\n",
    "    \n",
    "# test .forward_fast(). The output should be the exact same as from the .forward() method.\n",
    "net.forward_fast(sample_image)\n",
    "print(net.Z)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# test .calculate_loss(). This should print the loss, a scalar value\n",
    "net.calculate_loss(sample_image, sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
