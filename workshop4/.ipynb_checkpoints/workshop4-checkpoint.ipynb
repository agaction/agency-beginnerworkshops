{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 4: Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def get_data(path):\n",
    "    f = open(path, 'r')\n",
    "    \n",
    "    lines = f.readlines()\n",
    "    \n",
    "    training_images = np.zeros((len(lines), 784))\n",
    "    training_labels = np.zeros((len(lines), 10))\n",
    "    index = 0\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        label = int(line[0])\n",
    "        training_images[index, :] = np.fromstring(line[2:], dtype=int, sep=',')\n",
    "        training_labels[index, label - 1] = 1.0\n",
    "        index += 1\n",
    "        \n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    return training_images / 255, training_labels\n",
    "\n",
    "def sigmoid(x):\n",
    "    return np.where(x >= 0, \n",
    "                    1 / (1 + np.exp(-x)), \n",
    "                    np.exp(x) / (1 + np.exp(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, we implemented backpropagation and gradient descent for our neural network. This week, we will finally put everything together and train our network to read hand-written digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / Validation\n",
    "\n",
    "If you recall, we will need to show our network many different examples of numbers in order for it to learn effectively. The images we will show our network is known as the *training set*. We will also want some way to evaluate the performance of our network after it learns. For this, we will need a seperate set of data that the network has never seen before. This is the *validation set*. The training set should be larger than the validation set. In our case, there are 60000 image, label pairs in the training set, and 10000 image, label pairs in the validation set.\n",
    "\n",
    "The training process is simple. For every image in the training set, forward pass the image into the network, then backpropagate and update the weights. When we make it through the entire training set, we say that we have completed one *epoch*. Typically, we will need to train for several epochs -- that is to say, we often need to run through the training set multiple times.\n",
    "\n",
    "We also want to evaluate our network. For every image in the validation set, we will forward pass the image into the network. Then, we will look at the output, a vector of length 10 -- our network network is trained to place a 1 in the correct entry of the vector, and a 0 in all other entries. Therefore, the maximal entry in the network's output is the number that the network thinks the given image represents. We then compare the network's answer to the correct answer, the given label. After observing every image in the validation set, we note the proportion of the samples that the network got correct.\n",
    "\n",
    "Sometimes, it can be helpful to run through the validation set after every epoch, to see how the network's performance improves over time. This can be helpful for deciding how many epochs you want to run your network for.\n",
    "\n",
    "Also, in order to train our network effectively, we will need to pick an appropriate learning rate. If the learning rate is too large, the network will take too large steps during the gradient descent, and it will be ineffective and unstable. If the learning rate is too small, the training could be slow. We will find a good learning rate through trial and error. Remember, the learning rate should be a very small positive number, typically less than 1.\n",
    "\n",
    "Try to write your own training and validation methods. The code for forward pass and backpropagation is provided for you. See if you can get a validation accuracy of over 90 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \"\"\"\n",
    "    A Fully Connected Neural Network. There are 784 input layer nodes, 12 hidden layer nodes, and 10 output layer\n",
    "    nodes.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        \n",
    "        # Arrays to hold node values\n",
    "        self.N = np.zeros((784, ))\n",
    "        self.H = np.zeros((12, ))\n",
    "        self.Z = np.zeros((10, ))\n",
    "        \n",
    "        # Arrays to hold weight values (randomly initialized between -1 and 1)\n",
    "        self.W = 2 * np.random.rand(784, 12) - 1\n",
    "        self.V = 2 * np.random.rand(12, 10) - 1\n",
    "        \n",
    "        # Arrays to hold biases for hidden and output nodes\n",
    "        self.B = 2 * np.random.rand(12) - 1\n",
    "        self.C = 2 * np.random.rand(10) - 1\n",
    "        \n",
    "        \n",
    "        # Arrays to store values for calculating backprop efficiently\n",
    "        self.H_before = np.zeros((12, ))\n",
    "        self.Z_before = np.zeros((10, ))\n",
    "        \n",
    "        self.W_grad = np.zeros((784, 12))\n",
    "        self.V_grad = np.zeros((12, 10))\n",
    "        self.B_grad = np.zeros((12, ))\n",
    "        self.C_grad = np.zeros((10, ))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.N = x\n",
    "        self.H_before = np.dot(self.N, self.W) + self.B\n",
    "        self.H = np.tanh(self.H_before)\n",
    "        self.Z_before = np.dot(self.H, self.V) + self.C\n",
    "        self.Z = 1 / (1 + np.exp(-1 * self.Z_before))\n",
    "\n",
    "        \n",
    "    def calculate_loss(self, x, y):\n",
    "        out = self.forward(x)\n",
    "        loss = np.sum((self.Z - y) ** 2)\n",
    "        return loss\n",
    "\n",
    "        \n",
    "    def backpropagate(self, label):\n",
    "        dZ = -2 * (label - self.Z)\n",
    "        dZ_before = dZ * sigmoid(self.Z_before) * (1 - sigmoid(self.Z_before))\n",
    "        self.V_grad = np.outer(self.H, dZ_before)\n",
    "        self.C_grad = dZ_before\n",
    "        \n",
    "        dH = np.dot(dZ_before, np.transpose(self.V))\n",
    "        dH_before = dH * (1 / (np.cosh(self.H_before) ** 2))\n",
    "        self.W_grad = np.outer(self.N, dH_before)\n",
    "        self.B_grad = dH_before\n",
    "        \n",
    "    \n",
    "    def update(self, lr):\n",
    "        self.V -= lr * self.V_grad\n",
    "        self.C -= lr * self.C_grad\n",
    "        self.W -= lr * self.W_grad\n",
    "        self.B -= lr * self.B_grad\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Write a training method for the neural network. Add whatever method parameters you think you might need.\n",
    "        \"\"\"\n",
    "                \n",
    "            \n",
    "    def evaluate(self, val_images, val_labels):\n",
    "        \"\"\"\n",
    "        Write an evaluation method for the neural network. Add whatever method parameters you think you might need.\n",
    "        \"\"\"\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images, training_labels = get_data(\"mnist_train.csv\")\n",
    "val_images, val_labels = get_data(\"mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "After succesfully training, you may realize that our network has a difficult time getting accuracies higher than low nineties. From the testing I've done, that seems to be the best our basic network can do -- however, it has been shown that neural networks are capable of reaching accuracies over 99 percent for the MNIST dataset. What changes can we make to the structure of our network to accomplish this? More nodes in the hidden layer? Add another hidden layer? Perhaps there are even more sophisticated methods...\n",
    "\n",
    "Try to create a neural network that can achieve a validation accuracy of over 96 percent. It's probably easiest if you reuse code from the previous network. Feel free to try any method you can think of or find on the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
